# IMPLEMENTED

## Completed

- Added Pi-oriented model profile (`pi5`) with smaller core dimensions for edge deployment.
- Added co-evolution training mode with elite selection and mutation.
- Added curriculum schedules for remap severity and environment volatility in training.
- Added latent genetic memory persistence across episodes in training.
- Extended world simulation with environment controls:
  - wind vector
  - dynamic light source
  - force interaction and object movement
- Added reproducible embodiment schemas (`hexapod`, `car`, `drone`) and deterministic I/O remapping.
- Added high-complexity embodiment `polymorph120` (120 DOF; multiple of 10/6/8) and DOF reporting via `add-sim embodiments`.
- Added visualization command for single-run adaptation with environment control traces.
- Added side-by-side checkpoint comparison visualization under the same remap/environment schedule.
- Added dynamic quantized TorchScript export path.
- Added Pi-focused benchmark CLI.
- Added UDP hardware-in-the-loop adapter CLI.
- Added configurable memory gating research modes:
  - `sigmoid` baseline
  - `symplectic` (tanh + max pooling + learnable scale)
  - optional `top-k` memory slot paging
  - optional phase-aware gate modulation
  - optional DMD-inspired gate modulation
- Added parallel diverse-fitness training launcher (`add-train-parallel`).
- Extended training CLI with embodiment-aware transfer optimization:
  - `--embodiments`
  - `--enable-embodiment-transfer-loss`
  - `--transfer-loss-weight`
  - `--transfer-fitness-weight`
  - `--transfer-samples-per-step`
  - `--init-weights` warm-start support
- Added training performance controls for aggressive runs:
  - optional AMP (`--use-amp`)
  - TF32 toggle (`--allow-tf32`)
  - optional `torch.compile` (`--compile-model`, non-coevolution)
- Extended parallel launcher with mixed-device scheduling (`--device-pool`) and transfer-objective forwarding.
- Added symplectic/gating verification benchmark CLI (`add-gating-bench`) with long-horizon recovery score.
- Added structured run metric tracking (`*.metrics.json`) including active flags, per-epoch/generation performance (`mean_step_ms`), and fitness.
- Added parallel summary aggregation with flags/performance/fitness in `artifacts/parallel*/summary.json`.
- Added primitive principle tests in `tests/test_principles.py` for:
  - memory update from zero state
  - remap-driven adaptation signal shift
  - interconnected memory->I/O influence
  - top-k memory paging sparsity
  - environment control impact on stress/object motion
- Ran a longer diverse-fitness sweep in `artifacts/parallel-long` (8 variants, 30 epochs).
- Added cross-embodiment transfer evaluator (`add-cross-eval`) with scenario stressors and ranking output in `artifacts/cross-eval-summary.json`.
- Added transfer-focused tests in `tests/test_cross_eval.py` (recovery metric + rollout metric structure).
- Added cross-eval reporting utility (`add-cross-report`) to emit ranked Markdown/CSV with per-embodiment deltas.
- Extended `add-cross-report` to display ranking transfer score alongside unweighted transfer score and include embodiment-weight context.
- Added explicit checkpoint-list support in `add-cross-eval` for focused A/B/C comparisons.
- Added optional embodiment-weighted ranking in `add-cross-eval` (`--embodiment-weights`) while preserving unweighted score output for unbiased comparison.
- Ran focused long retrain (`artifacts/focused-variant03-long.pt`) and confirmed transfer improvement vs prior top checkpoints.
- Added hardy-line scenario profile support in cross-eval (`standard`, `hardy`, `extreme`) with stronger disturbance cases (`storm`, `blackout`, `crosswind`).
- Ran focused curriculum retrain and hardy-line ranking (`artifacts/cross-eval-hardy-focused-vs-top.json`) to compare robustness under harsher conditions.
- Ran additional curriculum sweep variants (`focused-curriculum-a`, `focused-curriculum-b`) and ranked them under hardy profile (`artifacts/cross-eval-hardy-curriculum-sweep.json`).
- Ran car-priority hardy ranking with embodiment weights (`artifacts/cross-eval-hardy-car-priority.json`) and confirmed the same champion remained top-ranked.
- Ran aggressive embodiment-aware parallel sweep (`artifacts/parallel-aggressive-cpu`) and ranked variants with/without `polymorph120` in hardy profiles.
- Ran warm-start focused fine-tune from `artifacts/focused-variant03-long.pt` to `artifacts/focused-variant03-long-poly-ft.pt`; improved hardy transfer and reduced mismatch:
  - 3-emb standard score: `0.32448 -> 0.34748`
  - 3-emb hardy score: `0.30826 -> 0.32899`
  - 3-emb hardy (car-priority weighted) score: `0.28209 -> 0.31125`
  - car mismatch: `1.48385 -> 1.25366`
  - generated comparison viz: `artifacts/focused-vs-polyft-car-hardy.gif`
- Enabled CUDA training environment in `.venv` using `torch==2.10.0+cu128` and executed GPU warm-start fine-tuning (`artifacts/focused-variant03-long-poly-ft-cuda.pt`).
- Ran mixed-device CUDA+CPU warm-start sweep (`artifacts/parallel-cuda-mixed-sweep`) and promoted a stable champion checkpoint (`artifacts/model-core-champion-v01.pt`).
- Confirmed new champion (`variant-01`) improvement over prior CUDA fine-tune (`focused-variant03-long-poly-ft-cuda`) with `runs_per_combo=2`:
  - standard score: `0.37217 -> 0.38302`
  - hardy score: `0.35110 -> 0.36079`
  - hardy car-priority score: `0.33477 -> 0.34523`
  - hardy car mismatch: `1.04158 -> 0.95847`
  - hardy poly4 score: `0.38086 -> 0.38828`
  - generated comparison viz: `artifacts/polyft-cuda-vs-v01-car-hardy.gif`
- Ran longer CUDA-heavy warm-start sweep (`artifacts/parallel-cuda-long-v02`) from prior champion and promoted `variant-01` as new top candidate (`artifacts/model-core-champion-v02.pt`):
  - standard score: `0.37877 -> 0.41071` (vs prior champion `artifacts/parallel-cuda-mixed-sweep/variant-01.pt`)
  - hardy score: `0.36234 -> 0.38989` (vs prior champion `artifacts/parallel-cuda-mixed-sweep/variant-01.pt`)
  - hardy car-priority score: `0.34798 -> 0.37822`
  - hardy poly4 score: `0.38907 -> 0.41016`
  - hardy overall mismatch: `0.67971 -> 0.54666`
  - hardy car mismatch: `0.91944 -> 0.71312`
  - generated comparison viz: `artifacts/cuda-long-v02-v01-car-crosswind.gif`
- Ran convergence-focused CUDA sweep (`artifacts/parallel-cuda-converge-v03`, 10 variants, 56 epochs, warm-start from `model-core-champion-v02`) and promoted `variant-07` as new champion (`artifacts/model-core-champion-v03.pt`):
  - hardy r6 score: `0.39073 -> 0.41967` (vs `model-core-champion-v02`)
  - hardy car-priority r6 score: `0.37992 -> 0.41226`
  - standard r3 score: `0.41668 -> 0.45133`
  - hardy poly4 r3 score: `0.41222 -> 0.43419`
  - hardy overall mismatch: `0.54291 -> 0.42497`
  - hardy car mismatch: `0.68657 -> 0.50892`
  - generated comparison viz: `artifacts/converge-v03-v07-vs-v02-car-crosswind.gif`
- Added champion-v03 metrics manifest: `artifacts/model-core-champion-v03.metrics.json`.
- Added strict device selection guard for training:
  - `add-train` now supports `--strict-device/--no-strict-device` (default strict).
  - `add-train-parallel` forwards strict mode to all variants.
  - requesting `--device cuda` now fails fast if CUDA is unavailable, preventing silent CPU fallback.
- Added device-guard tests in `tests/test_train_transfer.py` and kept suite passing (`21 passed`).
- Added tracking files (`TODO.md`, `IMPLEMENTED.md`) and updated backlog/docs.
- Added artifact interpretation reference (`docs/ARTIFACTS.md`) with concrete success thresholds, behavior expectations, and evaluation playbooks.
- Added embodiment profiling command (`add-sim profiler`) with JSON outputs for:
  - P50/P95 step latency
  - runtime memory usage (`state_mb`, `memory_tensor_mb`, `peak_gpu_alloc_mb`)
  - readiness/channel dynamism and memory-weight entropy
  - low/high usage control-channel indices for embodiment bottleneck inspection
- Added profiler tests in `tests/test_sim_profiler.py`.
- Extended `add-cross-eval` with train/heldout transfer harness features:
  - `--train-embodiments`
  - `--checkmate-threshold`
  - per-checkpoint `transfer_ratio_matrix` (source/train embodiments to all eval embodiments)
  - checkmate gate outputs (`checkmate_pass_all`, `checkmate_pass_heldout`, min/mean/heldout effectiveness)
- Extended `add-cross-report` with:
  - Checkmate gate table in Markdown
  - train/heldout split summary
  - best-checkpoint transfer ratio matrix block.
- Added cross-eval tests for:
  - subset/train embodiment resolution
  - transfer-ratio matrix semantics
  - checkmate gate computations.
- Ran explicit zero-shot transfer matrix/checkmate artifacts (`train={hexapod,car}`, `eval={hexapod,car,drone,polymorph120}`):
  - transfer-only: `artifacts/cross-eval-checkmate-transfer-hardy-poly4-r3.json`
  - transfer+capability: `artifacts/cross-eval-checkmate-capability-hardy-poly4-r3.json`
  - reports: corresponding `.md` and `.csv` files.
- Added observation-noise evaluation profiles in `add-cross-eval`:
  - `--noise-profile dropout-quant-v1` (moderate dropout/noise/8-bit quant)
  - `--noise-profile dropout-quant-v2` (aggressive dropout/noise/coarse quant)
- Ran high-repeat noisy checkmate validation (`threshold=0.95`, `runs_per_combo=6`) with `dropout-quant-v2`:
  - artifact: `artifacts/cross-eval-checkmate-transfer-noisyv2-hardy-poly4-r6-th095.json`
  - top models (`variant-01`, `variant-00`) passed `checkmate_pass_all=True`
  - prior champions (`model-core-champion-v03`, `variant-06`, `model-core-champion-v02`) failed `checkmate_pass_all`.
- Added capability-proxy harness in `add-cross-eval` (`--capability-profile bio-tech-v1`) with environment-tied biological/technological signal proxies and metrics:
  - `signal_reliability` (with raw correlation tracked as `signal_corr_raw`)
  - `signal_detection_auc` (with raw tracked as `signal_detection_auc_raw`)
  - `evasion_success`
  - blended `capability_score`
- Added optional blended ranking in cross-eval:
  - `overall_transfer_score = weighted_transfer + capability_score_weight * capability_score`
  - preserves transfer-only outputs alongside blended score.
- Extended `add-cross-report` to surface capability-enabled runs:
  - ranking score + weighted/unweighted transfer split
  - capability profile/weight context
  - top-5 capability proxy summary table.
- Added capability harness tests in `tests/test_cross_eval.py`:
  - `_binary_auc` behavior
  - capability-profile rollout on `polymorph120`
  - range checks for raw + normalized capability metrics.
- Restored CUDA runtime in project venv (`torch==2.10.0+cu130`) and verified RTX 5080 visibility.
- Ran capability-focused warm-start sweep `artifacts/parallel-cuda-capability-v01` (6 variants, 40 epochs, coevolution, transfer-aware, `hexapod/car/drone/polymorph120`) initialized from `artifacts/parallel-cuda-converge-v03/variant-06.pt`.
- New best checkpoints from this sweep:
  - transfer-only hardy poly4 (`runs_per_combo=3`): `variant-01` at `0.44643` (vs champion-v03 `0.43419`)
  - blended transfer+capability hardy poly4 (`capability_score_weight=0.25`): `variant-00` at `0.62127`
  - hardy `car` mismatch improved to `0.38388` (`variant-01`) and `0.40290` (`variant-00`) vs champion-v03 `0.46295`.
- Generated new comparisons:
  - `artifacts/capability-v01-v01-vs-v03-car-crosswind-thrust.gif`
  - `artifacts/capability-v01-v00-vs-v03-polymorph-storm.gif`

## Lessons Learned

- Detaching rollout memory between steps is required to avoid graph reuse errors in unrolled training.
- Returning dictionaries from model forward paths complicates export; wrappers with tensor/tuple outputs simplify TorchScript/ONNX.
- Visual comparisons are only meaningful when both models share identical initial state and remap/control schedules.
- Anonymous-channel adaptation still benefits from embodiment schemas when they are used only as reproducible mapping generators.
- Environment perturbations (wind/light/force) need to be explicit controls rather than implicit randomness to debug adaptation behavior.
- A lightweight HIL bridge is useful early, but production robotics needs transport-specific safety and timing guarantees.
- Symplectic-style gating and DMD/phase modulation are easiest to integrate when they preserve backward compatibility with the baseline memory API.
- Top-k slot paging is a practical first step toward manifold paging, but objective-level paging still needs dedicated work.
- Curriculum helps hard-regime robustness, but schedule parameters need tuning; in current hardy evaluation, non-curriculum focused training remained slightly better on transfer score.
- Curriculum variant A improved recovery but still trailed the non-curriculum focused model on overall hardy transfer score.
- Weighted ranking is useful for deployment-specific selection, but unweighted transfer should remain the primary generalization gate to avoid overfitting selection criteria to a single embodiment.
- In the current checkpoint set, car-priority weighting changed absolute score scale but not rank order; reducing car mismatch likely requires training/objective changes, not only selection weighting.
- Warm-starting a strong transfer checkpoint and optimizing explicit embodiment-mismatch terms gave materially better hardy transfer than broad scratch sweeps under the same resource budget.
- Transfer-focused loss can improve cross-embodiment metrics even when the legacy fitness scalar gets worse; selection needs transfer-centric metrics, not vitality-only proxies.
- On this hardware/software stack, `torch.compile` failed without Triton; disabling compile preserved strong performance gains from AMP/TF32 + objective tuning.
- `uv run` can resync to CPU-only torch from lockfile; running via `.venv\\Scripts\\python` with explicit CUDA wheels kept training on GPU.
- Guarding device selection in CLI is essential: strict `--device cuda` avoids wasting long runs on accidental CPU fallback when environments drift.
- High-repeat validation (`runs_per_combo` 4-6) is necessary before promoting champions; some gains shrink under more repeats, but robust winners keep rank across standard/hardy/car-priority profiles.
- Capability metrics in anonymous-channel settings must be polarity-invariant (raw negative correlation or AUC<0.5 can still indicate decodable signal if interpreted with sign inversion).
- Coevolution fitness rank still does not reliably predict cross-embodiment transfer/capability rank; promotion should stay tied to cross-eval artifacts, not training fitness alone.
- Per-embodiment profiling quickly reveals expected channel bottlenecks (e.g., `io_channels << control_dof`) and should be run before long sweeps to set realistic mapping-coverage expectations.
- A fixed `>=85%` checkmate threshold is useful as a floor gate, but current models already exceed it on the selected hardy split; tighter thresholds or harder heldout regimes are needed for discrimination.
- Harder noise regimes + stricter checkmate thresholds (`0.95`) create meaningful separation where `0.85` did not.
