# Experiment Log

Use compact entries:
- date:
- hypothesis:
- change:
- result:
- next action:

- date: 2026-02-13
- hypothesis: Embodiment-aware remap schedules plus explicit environment controls will make adaptation behavior easier to observe and compare.
- change: Added world environment controls (wind/light/force/object move), embodiment schemas, and single+comparison visualizer commands; added curriculum schedules and latent genetic memory in training.
- result: Smoke training and visualization completed; generated `artifacts/adaptation-controlled.gif` and `artifacts/adaptation-compare-controlled.gif` with remap events and adaptation traces.
- next action: Run longer RTX 5080 training and compare checkpoints across all three embodiments.
- date: 2026-02-13
- hypothesis: Longer parallel co-evolution with optional gating flags plus curriculum/genetic-memory toggles should produce a clearer best-performing variant and better run-to-run comparability.
- change: Added principle tests (`tests/test_principles.py`) and ran `add-train-parallel` equivalent with 8 variants, 30 epochs, mixed optional flags into `artifacts/parallel-long`.
- result: All 8 variants completed successfully and produced checkpoints, logs, and metrics summary (`artifacts/parallel-long/summary.json`). Primitive principle tests passed (5/5).
- next action: Rank top variants by fitness and generate side-by-side visualization comparisons for best 2-3 models across `hexapod/car/drone`.
- date: 2026-02-13
- hypothesis: Direct cross-embodiment ranking across shared scenario schedules will expose transfer-generalization differences better than single-fitness ranking.
- change: Added `add-cross-eval` (`src/ai_embedded_dynamic_diversity/train/cross_eval_cli.py`) and transfer tests (`tests/test_cross_eval.py`), then evaluated `artifacts/parallel-long`.
- result: `artifacts/cross-eval-summary.json` ranked `artifacts/parallel-long/variant-03.pt` highest by overall transfer score (~0.2961) across `hexapod/car/drone` under `mild/gust/force` scenarios.
- next action: Generate side-by-side visual comparisons for top-ranked checkpoints (`variant-03` vs `variant-02`) across each embodiment and inspect failure modes where mismatch spikes after remaps.
- date: 2026-02-13
- hypothesis: Top transfer-ranked checkpoints should show more consistent vitality under remaps across all embodiments.
- change: Generated side-by-side visual comparisons for top ranked models (`variant-03` vs `variant-02`) across `hexapod`, `car`, and `drone`.
- result: Produced `artifacts/cross-eval-top-hexapod.gif`, `artifacts/cross-eval-top-car.gif`, and `artifacts/cross-eval-top-drone.gif`; left model (`variant-03`) showed slightly higher final vitality in all three.
- next action: Build automated delta charts (mismatch/vitality) from `cross-eval-summary.json` and inspect per-remap recovery dips.
- date: 2026-02-13
- hypothesis: A focused long retrain on the best-transfer configuration should improve cross-embodiment transfer score beyond the current top checkpoint.
- change: Added `add-cross-report`, extended `add-cross-eval` with `--checkpoints-list`, trained `artifacts/focused-variant03-long.pt` (symplectic + topk + dmd + phase + coevolution + genetic memory), and evaluated against `variant-03` and `variant-02`.
- result: Focused model ranked #1 in `artifacts/cross-eval-focused-vs-top.json` with transfer score ~0.32448 (vs ~0.29614 and ~0.29556). Generated comparison report/csv and visualization `artifacts/focused-vs-old-top-hexapod.gif`.
- next action: Re-run focused training with curriculum enabled and compare whether robustness gains persist across harsher scenario mixes.
- date: 2026-02-13
- hypothesis: Curriculum-enabled focused training should improve transfer under hardy disturbance profiles.
- change: Added hardy/standard/extreme scenario profiles to `add-cross-eval`; generated focused-vs-old visualizations for `car` and `drone`; trained `artifacts/focused-variant03-curriculum.pt`; evaluated against focused and prior top checkpoints under hardy profile.
- result: Hardy ranking in `artifacts/cross-eval-hardy-focused-vs-top.json` kept `artifacts/focused-variant03-long.pt` #1 (~0.30826), curriculum model #2 (~0.30619), both above prior parallel tops (~0.281). Generated `artifacts/focused-vs-old-top-car.gif`, `artifacts/focused-vs-old-top-drone.gif`, and `artifacts/focused-curriculum-vs-focused-long-drone-hardy.gif`.
- next action: Tune curriculum schedules (start/end remap volatility) and re-test on hardy profile to close the remaining gap.
- date: 2026-02-13
- hypothesis: Curriculum schedule tuning may surpass the current focused non-curriculum champion on hardy-line transfer.
- change: Trained `focused-curriculum-a` and `focused-curriculum-b` variants, then evaluated a hardy sweep in `artifacts/cross-eval-hardy-curriculum-sweep.json`; generated comparison visualization `artifacts/focused-currA-vs-focused-long-car-hardy.gif`.
- result: Ranking stayed led by `artifacts/focused-variant03-long.pt` (~0.30826). Curriculum variants reached ~0.30619 (`focused-variant03-curriculum`), ~0.30564 (`focused-curriculum-a`), and ~0.29824 (`focused-curriculum-b`). All focused variants outperformed prior `parallel-long/variant-03` (~0.28134).
- next action: Target car-specific robustness (crosswind/force) and consider embodiment-aware objective weighting in transfer score.
- date: 2026-02-17
- hypothesis: Embodiment-weighted ranking (`car` priority) may change hardy-line checkpoint selection and better reflect near-term deployment priorities.
- change: Added `--embodiment-weights` support to `add-cross-eval`, added parser/weighting tests, ran weighted hardy evaluation in `artifacts/cross-eval-hardy-car-priority.json` with `hexapod=1,car=2.5,drone=1`, and generated report outputs (`.md`, `.csv`).
- result: Top checkpoint did not change; `artifacts/focused-variant03-long.pt` remained #1 (`weighted ~0.29343`, `unweighted ~0.31016`). Car transfer remained the limiting factor despite weighted selection.
- next action: Add car-focused objective weighting/sampling during training (storm/crosswind emphasis), then rerun hardy weighted+unweighted comparisons.
- date: 2026-02-17
- hypothesis: Explicit embodiment-mismatch optimization during training (including new high-DOF stress embodiment) should improve transferable adaptation, especially car mismatch under hardy stressors.
- change: Added `polymorph120` embodiment, added embodiment-aware transfer loss/fitness terms and warm-start support to training (`--embodiments`, `--enable-embodiment-transfer-loss`, `--transfer-loss-weight`, `--transfer-fitness-weight`, `--transfer-samples-per-step`, `--init-weights`), and ran:
  - aggressive scratch sweep: `artifacts/parallel-aggressive-cpu`
  - warm-start fine-tune from champion: `artifacts/focused-variant03-long-poly-ft.pt`
- result: Scratch aggressive variants did not beat champion on legacy hardy score. Warm-start fine-tune did beat champion:
  - `artifacts/cross-eval-focused-vs-polyft-hardy.json`: `0.32899` vs `0.30826`
  - `artifacts/cross-eval-focused-vs-polyft-standard.json`: `0.34748` vs `0.32448`
  - `artifacts/cross-eval-focused-vs-polyft-hardy-car-priority.json`: weighted score `0.31125` vs `0.28209`
  - car mismatch improved from `1.48385` to `1.25366`
  - 4-emb hardy score (with `polymorph120`) also improved (`0.36389` vs `0.34795`)
- next action: Run car-priority weighted hardy eval including `polymorph120` and tune transfer-fitness weight so proxy fitness aligns better with transfer improvements.
- date: 2026-02-17
- hypothesis: Moving the warm-start + transfer-objective pipeline to CUDA and running a mixed device sweep should reduce mismatch further while improving hardy transfer.
- change: Installed CUDA PyTorch (`2.10.0+cu128`) in `.venv`, trained `artifacts/focused-variant03-long-poly-ft-cuda.pt`, then ran mixed-device warm-start sweep `artifacts/parallel-cuda-mixed-sweep` and evaluated top checkpoints with `runs_per_combo=2`.
- result: `artifacts/parallel-cuda-mixed-sweep/variant-01.pt` became new best across standard/hardy/car-priority/poly4:
  - standard: `0.38302` (prev `0.37217`)
  - hardy: `0.36079` (prev `0.35110`)
  - hardy car-priority: `0.34523` (prev `0.33477`)
  - hardy car mismatch: `0.95847` (prev `1.04158`)
  - hardy poly4: `0.38828` (prev `0.38086`)
  - comparison viz: `artifacts/polyft-cuda-vs-v01-car-hardy.gif`
- next action: Lock reproducible CUDA dependency workflow for `uv`, then run a longer RTX-only sweep using `variant-01` as warm-start with stronger storm/crosswind weighting.
- date: 2026-02-17
- hypothesis: A longer CUDA-heavy warm-start sweep with stronger transfer pressure (`transfer_loss_weight=0.5`, `transfer_fitness_weight=0.16`, `samples=4`) can reduce hardy mismatch and especially car mismatch below `0.9`.
- change: Ran `artifacts/parallel-cuda-long-v02` (`8` variants, `48` epochs, warm-start from `artifacts/parallel-cuda-mixed-sweep/variant-01.pt`), then strict cross-evals (`runs_per_combo=2`) on hardy, hardy car-priority, and hardy poly4. Added strict device guard (`--strict-device`) to training CLIs with tests.
- result: `artifacts/parallel-cuda-long-v02/variant-01.pt` became new best across all hardy evaluations:
  - standard: `0.41071` vs old champion `0.37877`
  - hardy: `0.38989` vs old champion `0.36234`
  - hardy car-priority: `0.37822` vs `0.34798`
  - hardy poly4: `0.41016` vs `0.38907`
  - hardy overall mismatch: `0.54666` vs `0.67971`
  - hardy car mismatch: `0.71312` vs `0.91944` (crossed target `<0.9`)
  - comparison viz: `artifacts/cuda-long-v02-v01-car-crosswind.gif` (`final_mismatch 0.556` vs `0.633`)
- next action: Push car hardy mismatch below `0.70` with lower variance across repetitions and standardize CUDA lock/index handling in `uv` so plain `uv run` remains GPU-backed.
- date: 2026-02-17
- hypothesis: A longer convergence-focused warm-start sweep plus higher-repeat ranking (`runs_per_combo=4..6`) should improve hardy transfer while confirming stability beyond short-run noise.
- change: Reinstalled CUDA torch in `.venv` (`cu130`), ran `artifacts/parallel-cuda-converge-v03` (10 variants, 56 epochs, warm-start from `artifacts/model-core-champion-v02.pt`), then evaluated top candidates with high-repeat validations:
  - `artifacts/cross-eval-cuda-converge-v03-top5-hardy-r4.json`
  - `artifacts/cross-eval-cuda-converge-v03-top5-hardy-car-priority-r4.json`
  - `artifacts/cross-eval-cuda-converge-v03-top5-standard-r3.json`
  - `artifacts/cross-eval-cuda-converge-v03-top5-hardy-poly4-r3.json`
  - `artifacts/cross-eval-cuda-converge-v03-top4-hardy-r6.json`
  - `artifacts/cross-eval-cuda-converge-v03-top4-hardy-car-priority-r6.json`
- result: `artifacts/parallel-cuda-converge-v03/variant-07.pt` stayed #1 across all profiles/repeat levels and was promoted to `artifacts/model-core-champion-v03.pt`:
  - hardy r6: `0.41967` vs champion-v02 `0.39073`
  - hardy car-priority r6: `0.41226` vs `0.37992`
  - standard r3: `0.45133` vs `0.41668`
  - hardy car mismatch (r6): `0.50892` vs `0.68657`
  - comparison viz: `artifacts/converge-v03-v07-vs-v02-car-crosswind.gif`
- next action: Target `<0.50` hardy car mismatch and reduce car transfer variance under `storm/crosswind` without regressing standard transfer.
- date: 2026-02-17
- hypothesis: Adding explicit emergent-capability tracks (signaling, signal detection, evasion, conjoining) as measured objectives can accelerate robustness and adaptability beyond pure transfer-score optimization.
- change: Added roadmap items and metric definitions for:
  - emergent signaling and signal detection under anonymous channels
  - threat evasion under disturbance/adversarial scenarios
  - genetic+memory survival over multi-generation runs
  - conjoining behavior (environment/tool use + bonding/formation proxies)
- result: Work is staged in TODO/backlog with measurable outputs to integrate into future training/eval loops without forcing explicit imitation policies.
- next action: Implement first eval harness slice (`signal_reliability`, `signal_detection_auc`, `evasion_success`) and run against champion-v03 baseline.
- date: 2026-02-17
- hypothesis: Environment-tied biological/technological proxy signals in cross-eval will expose emergent signaling/detection/evasion behavior without hardcoding explicit behavior labels.
- change: Extended `add-cross-eval` with `--capability-profile bio-tech-v1` and blended ranking (`--capability-score-weight`), then added capability-aware reporting and tests.
- result: Capability metrics are now emitted per run/embodiment (`signal_corr_raw`, `signal_reliability`, `signal_detection_auc_raw`, `signal_detection_auc`, `evasion_success`, `capability_score`). Initial baseline showed polarity sensitivity artifacts, so scoring was updated to be polarity-invariant (`abs(corr)`, `max(auc, 1-auc)`), aligned with anonymous-channel semantics.
- next action: Run a new CUDA warm-start sweep and compare transfer-only vs blended capability ranking across all four embodiments.
- date: 2026-02-17
- hypothesis: Warm-starting from `variant-06` with strong transfer pressure and poly4 curriculum will improve both hardy transfer and capability proxies, especially `car` mismatch.
- change: Ran `artifacts/parallel-cuda-capability-v01` (`6` variants, `40` epochs, `hexapod/car/drone/polymorph120`, transfer loss/fitness enabled, warm-start from `artifacts/parallel-cuda-converge-v03/variant-06.pt`) and evaluated with/without capability profile under hardy poly4 (`runs_per_combo=3`).
- result:
  - transfer-only best: `variant-01` score `0.44643` (vs champion-v03 `0.43419`)
  - blended transfer+capability best (`weight=0.25`): `variant-00` score `0.62127`
  - hardy `car` mismatch improved to `0.38388` (`variant-01`) and `0.40290` (`variant-00`) vs champion-v03 `0.46295`
  - generated behavior comparisons: `artifacts/capability-v01-v01-vs-v03-car-crosswind-thrust.gif`, `artifacts/capability-v01-v00-vs-v03-polymorph-storm.gif`
- next action: Validate `variant-00` and `variant-01` with higher-repeat (`runs_per_combo >= 6`) standard+hardy+car-priority checks before champion promotion; add explicit mimicry/conjoining proxies to capability harness.
- date: 2026-02-17
- hypothesis: A dedicated embodiment profiler will expose low-utilization channels, readiness saturation, and runtime pressure early, accelerating transfer/debug loops.
- change: Added `add-sim profiler` in `src/ai_embedded_dynamic_diversity/sim/cli.py` with metrics JSON output and tests (`tests/test_sim_profiler.py`), then ran a CUDA profile on `polymorph120` into `artifacts/profiler-polymorph120-v01.json`.
- result: Profiler reports expected bottleneck and dynamism signals:
  - latency: `p50 ~3.16ms`, `p95 ~3.98ms`
  - peak GPU alloc: `~11.54MB`
  - channel firing fraction: `~0.133`
  - memory entropy near-uniform (`~0.99999`)
  - mapping coverage `~0.133` (expected from `io_channels=16` over `DOF=120`)
- next action: Use profiler outputs to drive online-remap and noisy-signal training priorities; add transfer-matrix and checkmate suite as next validation gates.
- date: 2026-02-17
- hypothesis: A checkmate gate plus explicit transfer-ratio matrix (`train={hexapod,car}`, heldout `{drone,polymorph120}`) will provide a hard acceptance test and clearer zero-shot transfer diagnostics.
- change: Extended `add-cross-eval` with `--train-embodiments` and `--checkmate-threshold`, added per-checkpoint `transfer_ratio_matrix` and checkmate pass/effectiveness outputs; updated `add-cross-report` to render checkmate tables and matrix for best checkpoint; added unit tests.
- result:
  - transfer-only eval: `artifacts/cross-eval-checkmate-transfer-hardy-poly4-r3.json` (best: `variant-01`)
  - capability-weighted eval: `artifacts/cross-eval-checkmate-capability-hardy-poly4-r3.json` (best: `variant-00`)
  - all compared checkpoints passed `checkmate_threshold=0.85` on this split (`min effectiveness ~0.94+`), indicating the gate is currently permissive for top candidates.
- next action: raise checkmate threshold and/or harden heldout conditions (e.g., extreme scenarios, added noise/dropout) to improve ranking discrimination before champion promotion.
- date: 2026-02-17
- hypothesis: Stricter checkmate threshold (`0.95`) plus stronger sensor corruption will make the gate discriminative for champion selection.
- change: Added cross-eval noise profiles (`dropout-quant-v1`, `dropout-quant-v2`), then ran high-repeat (`runs_per_combo=6`) hardy checkmate with `train={hexapod,car}` and `noise_profile=dropout-quant-v2`.
- result:
  - artifact: `artifacts/cross-eval-checkmate-transfer-noisyv2-hardy-poly4-r6-th095.json`
  - `variant-01` remained #1 (`0.44526`) and `variant-00` #2 (`0.44224`)
  - only top-2 passed `checkmate_pass_all=True`; prior champions failed full-checkmate at threshold `0.95`
  - heldout check (`drone,polymorph120`) remained pass for all candidates, so weakest link remains in-train embodiment floor (`car`).
- next action: integrate noisy-signal curriculum into training (not only eval) and re-test whether champion-v03 line closes the `car` robustness gap under noisy checkmate.
